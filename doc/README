>>> MULTIWORD TERM TOOLKIT <<<

Version 0.3
Released: Aug 27, 2010
Author: Carlos Ramisch
Contact: carlosramisch@gmail.com

CAUTION: DOCUMENTATION IS NOT UP TO DATE WITH THE LAST VERSION OF THE TOOLKIT

The goal of the MWE Toolkit is to perform automatic Multiword Term extraction
from domain-specific documents. Please read this documentation before you start
the work.

    1) DOCUMENT TYPE DEFINITIONS

You will find three different DTD files that describe the format of the three
inputs of the MWT toolkit scripts:

* mwttoolkit-corpus.dtd

A corpus is a pre-processed sequence of sentences, each word has a surface form,
an optional lemma and an optional Part Of Speech. It is highly recommended that
you lemmatise and POS tag your corpus before you start to use MWT toolkit.

* mwttoolkit-patterns.dtd

A list of patterns for candidate extraction. A pattern is an ngram with wildcard
words that will match ngrams in the corpus. An example of patterns generated for
the Genia corpus can be found in the "examples" folder.

* mwttoolkit-candidates.dtd

A list of MWT candidates. This is the basic intermediary format used to
manipulate the candidates, calculate their features, etc. You can convert this
XML format easily into arff (WEKA) by calling the script that does this.

    2) EXAMPLES

The "toy" folder contains a set of files for performing a toy experiment.
You could try to run the whole pipeline by calling "sh runAll.sh" The specific
documentation about the examples is in a dedicated README file.

    3) SCRIPTS

The "bin" folder contains a set of useful scripts that automatise part of the
MWT extraction process:

* config.py
    
    Common configuration parameters and options for mwttoolkit. You MUST set
    the appropriate values for your experimental setup before starting your 
    work. Please pay special attention to inform the correct path to the Yahoo
    python API, which can be downloaded at ???URL???

* candidates.py

    This script extract Multiword Term candidates from a raw corpus in valid XML
    (mwttoolkit-corpus.dtd) and generates a candidate list in valid XML
    (mwttoolkit-candidates.dtd). There are two possible extraction methods: the
    -p option assumes that you have a patterns file in which you define 
    shallow morphosyntactic patterns (e.g. I want to extract Verb + Preposition
    pairs); the -n option assumes that you do not care about the type of the
    candidates and that you are trying to extract all possible ngrams from the
    corpus. Notice that in the -n option, ngrams are not extracted across
    sentence borders, since these would probably not be interesting MWT 
    candidates.
    
* filter.py    

    This script filters the candidate list based:
        1) On the number of occurrences of the candidate. The threshold might
        be defined individually for each corpus. Candidates occurring less than
        the threshold are filtered out.
        2) On the order of the candidates. Only the top n candidates are kept in
        the list. This operation is called "crop" the candidate list.
    
* index.py
    
    This script creates an index file for a given corpus. The index file an
    inverted word index that says how many occurrences of each word were found
    in the corpus. If not explicitely declared, the default is to count lemmas
    instead of word occurrences.
    
* counter.py

    This script calculates individual word frequencies for a given candidate 
    list in a given corpus. The corpus may be a valid corpus word index 
    generated by the `index.py` script (-i option) or it may be the World Wide 
    Web through Yahoo's Web Search interface (-y option). Notice that this
    script does not calculate joint ngram frequencies. Joint ngram frequencies
    are provided by the `candidates.py` script, but you cannot calculate the
    joint frequency of an ngram in another corpus that is not the corpus in 
    which it was found. This will be implemented in future versions.
    
* feat_association.py

    This script adds four new features for each candidate in the list and for
    each corpus with a known size. These features correspond to Association
    Measures (AMs) based on the frequency of the ngram compared to the frequency
    of the individual words. The AMs are:
        mle: Maximum Likelihood Estimator
        pmi: Pointwise Mutual Information
        t: Student's t test score
        dice: Dice's coeficient
    Each AM feature is subscribed by the name of the corpus from which it was
    calculated.
    
* feat_pattern.py

    This script adds two new features for each candidate in the list. These two
    features correspond to the POS pattern and to the number of words in the
    candidate base form. The former is the sequence of Part Of Speech tags in
    the candidate, for example, a sequence of Nouns or Adjectives. The latter is
    the value of n in the ngram that describes the base for of the candidate.
    
* eval_automatic.py

    This script performs the automatic annotation of a candidate list according
    to a reference list (also called Gold Standard). The reference list should 
    contain a manually verified list of attested Multiword Terms of the domain.
    The annotation defines a True Positive class for each candidate, which is
    True if the candidate occurs in the reference and False if the candidate is
    not in the reference (thus the candidate is probably a random word 
    combination and not a MWT).
    
* xml2arff.py

    This script converts a candidates file in XML (mwttoolkit-candidates.dtd) 
    into a corresponding representation in the arff file format, used by the
    WEKA machine learning toolkit. Only features and TP classes are considered,
    information about the candidate's ngrams or occurrences are ignored. Please
    notice that if you don't have a feature that uniquely identifies your 
    candidate, you will not be able to trace back the classifier results to the
    original candidates. 
    
* wc.py

    This script simply gives some stats about a candidates file, such as number
    of candidates, etc. Output is written on stderr.

* sort.py  

    This script sorts the candidate list according to the value of a feature (or
    the values of some features) that is/are called key feature(s). The key is
    used to sort the candidates in descending order (except if explicitely asked
    to sort in ascending order). Sorting is stable, i.e. if two candidates have 
    the same key feature values, their relative order will be preserved in the 
    output.
    
Remark: for more information, call the scripts with no parameter and read the
usage instructions. 
