#!/usr/bin/python
# -*- coding:UTF-8 -*-

################################################################################
#
# Copyright 2010 Carlos Ramisch
#
# counter.py is part of mwetoolkit
#
# mwetoolkit is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# mwetoolkit is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with mwetoolkit.  If not, see <http://www.gnu.org/licenses/>.
#
################################################################################
"""
    This script calculates individual word frequencies for a given candidate 
    list in a given corpus. The corpus may be a valid corpus word index 
    generated by the `index.py` script (-i option) or it may be the World Wide 
    Web through Yahoo's Web Search interface (-y option) or Google's Web Search
    interface (-w option). Notice that this script does not calculate joint 
    ngram frequencies. Joint ngram frequencies are provided by the 
    `candidates.py` script, but you cannot calculate the joint frequency of an 
    ngram in another corpus that is not the corpus in which it was found. This 
    will be implemented in future versions.

    For more information, call the script with no parameter and read the
    usage instructions.
"""

import sys
import getopt
import shelve
import xml.sax
import pdb
import array
import re

from xmlhandler.candidatesXMLHandler import CandidatesXMLHandler
from xmlhandler.classes.__common import WILDCARD, CORPUS_SIZE_KEY, SEPARATOR                                        
from xmlhandler.classes.frequency import Frequency
from xmlhandler.classes.yahooFreq import YahooFreq
from xmlhandler.classes.googleFreq import GoogleFreq
from xmlhandler.classes.corpus_size import CorpusSize
#from xmlhandler.classes.corpus import Corpus
#from xmlhandler.classes.suffix_array import SuffixArray
from util import usage, read_options, treat_options_simplest, verbose
    
################################################################################
# GLOBALS    
    
usage_string = """Usage: 
    
python %(program)s [-y | -i <corpus.index>] OPTIONS <candidates.xml>

-i <corpus.index> OR --index <corpus.index>
    Name of the index file that that was created by "index.py" from a corpus
    file. The <corpus.index> file contains the frequencies of individual words.

-y OR --yahoo
    Search for frequencies in the Web using Yahoo Web Search as approximator for
    Web document frequencies.    
    
-w OR --google
    Search for frequencies in the Web using Google Web Search as approximator 
    for Web document frequencies.   
    
OPTIONS may be:

-g OR --ignore-pos
     Ignores Part-Of-Speech when counting candidate occurences. This means, for
     example, that "like" as a preposition and "like" as a verb will be counted 
     as the same entity. If you are using -y, this option will be ignored. 
     Default false.
     
-v OR --verbose
    Print messages that explain what is happening.     
   
    The <candidates.xml> file must be valid XML (mwetoolkit-candidates.dtd).
You must chose either the -y option or the -i otpion, both are not allowed at 
the same time. 
"""    
corpus_file = array.array( 'L' )        
ngrams_file = array.array( 'L' )
vocab_file = {}
get_freq_function = None
freq_name = "?"
web_freq = None
the_corpus_size = -1
entity_counter = 0
     
################################################################################
       
def treat_meta( meta ) :
    """
        Adds a `CorpusSize` meta-information to the header and prints the 
        header. The corpus size is important to allow the calculation of 
        statistical Association Measures by the `feat_association.py` script.
        
        @param meta The `Meta` header that is being read from the XML file.        
    """
    global freq_name, the_corpus_size
    meta.add_corpus_size( CorpusSize( name=freq_name, value=the_corpus_size ) )
    print meta.to_xml()
       
################################################################################
       
def treat_candidate( candidate ) :
    """
        For each candidate, searches for the individual word frequencies of the
        base ngram. The corresponding function, for a corpus index or for yahoo,
        will be called, depending on the -i or -y options. The frequencies are
        added as a child element of the word and then the candidate is printed.
        
        @param candidate The `Candidate` that is being read from the XML file.        
    """
    global get_freq_function, freq_name, entity_counter
    if entity_counter % 100 == 0 :
        verbose( "Processing ngram number %(n)d" % { "n":entity_counter } )
    (c_surfaces, c_lemmas, c_pos ) = ( [], [], [] )
    for w in candidate :
        c_surfaces.append( w.surface )
        c_lemmas.append( w.lemma )
        c_pos.append( w.pos )       
        freq_value = get_freq_function( [ w.surface ], [ w.lemma ], [ w.pos ] )
        w.add_frequency( Frequency( freq_name, freq_value ) )
    # Global frequency
    freq_value = get_freq_function( c_surfaces, c_lemmas, c_pos )
    candidate.add_frequency( Frequency( freq_name, freq_value ) )
    print candidate.to_xml().encode( 'utf-8' )
    entity_counter += 1

################################################################################
       
def compare_ngram_index( corpus, pos, ngram_ids ) :
    """
        returns 1 if ngram(pos1) >lex ngram(pos), -1 for ngram(pos1) <lex
        ngram(pos) and 0 for matching ngrams
    """   
    pos1_cursor = pos
    wordp1 = corpus[ pos1_cursor ]
    pos2_cursor = 0 
    wordp2 = ngram_ids[ pos2_cursor ] 
    
    while wordp1 == wordp2 and pos2_cursor < len( ngram_ids ) - 1:
        # both are zero, we can stop because they are considered identical
        if wordp1 == 0 :
            break                
        pos1_cursor += 1   
        wordp1 = corpus[ pos1_cursor ]
        pos2_cursor += 1   
        wordp2 = ngram_ids[ pos2_cursor ]             
    return wordp1 - wordp2      

################################################################################

def binary_search( ng_ids, sufarray, corpus, gt_fct ) :
    """
    """
    i_low = 0
    i_up = len( sufarray )
    i_mid = ( i_up + i_low ) / 2
    while i_up - i_low > 1 :                              
        if gt_fct( compare_ngram_index( corpus, sufarray[i_mid], ng_ids ), 0 ) :
            i_up = i_mid
        else :
            i_low = i_mid
        i_mid = ( i_up + i_low ) / 2
    return i_mid
    
################################################################################     
       
def get_freq_index( surfaces, lemmas, pos ) :
    """
        Gets the frequency (number of occurrences) of a token (word) in the
        index file. Calling this function assumes that you called the script
        with the -i option and with a valid index file.
        
        @param token A string corresponding to the surface form or lemma
        of a word.
        
        @param pos A string corresponding to the Part Of Speech of a word.
    """
    global build_entry, ngrams_file, corpus_file, vocab_file
    ng_ids = []
    #pdb.set_trace()
    for i in range( len( surfaces ) ) :
        w = build_entry( surfaces[ i ], lemmas[i], pos[ i ] )
        w_id = vocab_file.get( w, None )
        if w_id :
            ng_ids.append( w_id )
        else :
            return 0
    i_last = binary_search( ng_ids, ngrams_file, corpus_file, lambda a, b: a > b )
    i_first = binary_search( ng_ids, ngrams_file, corpus_file, lambda a, b: a >= b ) 
    return i_last - i_first   
    
################################################################################

def get_freq_web( surfaces, lemmas, pos ) : 
    """
        Gets the frequency (number of occurrences) of a token (word) in the
        Web through Yahoo's or Google's index. Calling this function assumes 
        that you called the script with the -y or -w option.
        
        @param token A list corresponding to the surface forms or lemmas of a
        word.
        
        @param pos A list corresponding to the Part Of Speeches of a word. This
        parameter is ignored since Web search engines dos no provide linguistic 
        information.
    """
    # POS is ignored
    global web_freq, build_entry
    search_term = ""    
    for i in range( len( surfaces ) ) :
        search_term = search_term + build_entry( surfaces[ i ], lemmas[i], pos[ i ] ) + " "   
    return web_freq.search_frequency( search_term.strip() )

################################################################################

def load_array_from_file( an_array, a_filename ) :
    """
    """
    MAX_MEM = 10000
    fd = open( a_filename )
    isMore = True
    while isMore :
        try :    
            an_array.fromfile( fd, MAX_MEM )
        except EOFError :
            isMore = False # Did not read MAX_MEM_ITEMS items? Not a problem...
    fd.close()

################################################################################

def open_index( prefix ) :
    """
        Open the index files (valid index created by the `index3.py` script). 
                
        @param index_filename The string name of the index file.
    """
    global vocab_file, ngrams_file, corpus_file, freq_name, the_corpus_size
    try :      
        verbose( "Loading index files... this may take some time." )
        verbose( "Loading .vocab file" )
        vocab_fd = shelve.open( prefix + ".vocab" )
        vocab_file.update( vocab_fd )
        vocab_fd.close()        
        verbose( "Loading .corpus file" )
        load_array_from_file( corpus_file, prefix + ".corpus" )
        verbose( "Loading .ngrams file" )
        load_array_from_file( ngrams_file, prefix + ".ngrams" )         
        freq_name = re.sub( ".*/", "", prefix )
        #pdb.set_trace()
        the_corpus_size = vocab_file[ CORPUS_SIZE_KEY ]              
    except IOError :        
        print >> sys.stderr, "Error opening the index."
        print >> sys.stderr, "Try again with another index filename."
        sys.exit( 2 )
    except KeyError :        
        print >> sys.stderr, "Error opening the index."
        print >> sys.stderr, "Try again with another index filename."
        sys.exit( 2 )        

################################################################################

def treat_options( opts, arg, n_arg, usage_string ) :
    """
        Callback function that handles the command line options of this script.
        
        @param opts The options parsed by getopts. Ignored.
        
        @param arg The argument list parsed by getopts.
        
        @param n_arg The number of arguments expected for this script.    
    """
    global cache_file, get_freq_function, freq_name, build_entry, web_freq, \
           the_corpus_size
    surface_flag = False
    pos_flag = False
    mode = []
    for ( o, a ) in opts:
        if o in ( "-i", "--index" ) : 
            open_index( a )
            get_freq_function = get_freq_index
            mode.append( "index" )              
        elif o in ( "-y", "--yahoo" ) :
            web_freq = YahooFreq()          
            freq_name = "yahoo"
            pos_flag = True 
            the_corpus_size = web_freq.corpus_size()         
            get_freq_function = get_freq_web
            mode.append( "yahoo" )   
        elif o in ( "-w", "--google" ) :
            web_freq = GoogleFreq()          
            freq_name = "google"
            pos_flag = True 
            the_corpus_size = web_freq.corpus_size()         
            get_freq_function = get_freq_web
            mode.append( "google" ) 
        elif o in ("-s", "--surface" ) :
            surface_flag = True
        elif o in ("-g", "--ignore-pos"): 
            pos_flag = True
                 
    if mode == [ "index" ] :       
        if surface_flag and pos_flag :
            build_entry = lambda s, l, p: (s + SEPARATOR + WILDCARD).encode('utf-8')
        elif surface_flag :
            build_entry = lambda s, l, p: (s + SEPARATOR + p).encode('utf-8')
        elif pos_flag :
            build_entry = lambda s, l, p: (l + SEPARATOR + WILDCARD).encode('utf-8')
        else :      
            build_entry = lambda s, l, p: (l + SEPARATOR + p).encode('utf-8')
    else : # Web search, entries are single surface or lemma forms         
        if surface_flag :
            build_entry = lambda s, l, p: s.encode('utf-8')
        else :
            build_entry = lambda s, l, p: l.encode('utf-8')
        
    if len(mode) != 1 :
        print >> sys.stderr, "Exactly one option -y, -w or -i, must be provided"
        usage( usage_string )
        sys.exit( 2 )
                
    treat_options_simplest( opts, arg, n_arg, usage_string )

################################################################################
# MAIN SCRIPT

longopts = ["yahoo", "google", "index=", "verbose", "ignore-pos", "surface" ]
arg = read_options( "ywi:vgs", longopts, treat_options, 1, usage_string )  

try :    
    print """<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE candidates SYSTEM "dtd/mwetoolkit-candidates.dtd">
<candidates>
""" 
    input_file = open( arg[ 0 ] )        
    parser = xml.sax.make_parser()
    parser.setContentHandler(CandidatesXMLHandler(treat_meta, treat_candidate)) 
    verbose( "Counting ngrams in candidates file" )
    parser.parse( input_file )
    input_file.close() 
    print "</candidates>"      
    
except IOError, err :
    print >> sys.stderr, err
except Exception, err :
    print >> sys.stderr, err
    print >> sys.stderr, "You probably provided an invalid candidates file," + \
                         " please validate it against the DTD " + \
                         "(dtd/mwetoolkit-candidates.dtd)"
finally :
    if web_freq :
        web_freq.flush_cache() # VERY IMPORTANT!
