#!/usr/bin/python
"""
    This script calculates individual word frequencies for a given candidate 
    list in a given corpus. The corpus may be a valid corpus word index 
    generated by the `index.py` script (-i option) or it may be the World Wide 
    Web through Yahoo's Web Search interface (-y option) or Google's Web Search
    interface (-w option). Notice that this script does not calculate joint 
    ngram frequencies. Joint ngram frequencies are provided by the 
    `candidates.py` script, but you cannot calculate the joint frequency of an 
    ngram in another corpus that is not the corpus in which it was found. This 
    will be implemented in future versions.

    For more information, call the script with no parameter and read the
    usage instructions.
"""

import sys
import getopt
import shelve
import xml.sax
import os
import tempfile
import pdb
import os.path

from xmlhandler.candidatesXMLHandler import CandidatesXMLHandler
from xmlhandler.corpusXMLHandler import CorpusXMLHandler
from xmlhandler.classes.__common import WILDCARD, \
                                        CORPUS_SIZE_KEY, \
                                        INDEX_NAME_KEY, \
                                        TEMP_PREFIX, \
                                        TEMP_FOLDER                                  
from xmlhandler.classes.frequency import Frequency
from xmlhandler.classes.candidate import Candidate
from xmlhandler.classes.yahooFreq import YahooFreq
from xmlhandler.classes.googleFreq import GoogleFreq
from xmlhandler.classes.corpus_size import CorpusSize
from util import usage, read_options, treat_options_simplest, verbose, \
                 set_verbose
    
################################################################################
# GLOBALS    
    
usage_string = """Usage: 
    
python %(program)s [-y | -i <corpus.index>] OPTIONS <candidates.xml>

-i <corpus.index> OR --index <corpus.index>
    Name of the index file that that was created by "index.py" from a corpus
    file. The <corpus.index> file contains the frequencies of individual words.

-y OR --yahoo
    Search for frequencies in the Web using Yahoo Web Search as approximator for
    Web document frequencies.    
    
-w OR --google
    Search for frequencies in the Web using Google Web Search as approximator 
    for Web document frequencies.   
    
OPTIONS may be:

-g OR --ignore-pos
     Ignores Part-Of-Speech when counting candidate occurences. This means, for
     example, that "like" as a preposition and "like" as a verb will be counted 
     as the same entity. If you are using -y, this option will be ignored. 
     Default false.
     
 -f <lowerbound> OR --from <lowerbound>
    Start to count the candidates at position <lowerbound>. The counter starts
    at zero, meaning that "-f 1" will start at the second candidate. The value
    should be positive and lower than the upper bound (if any).
    
 -t <upperbound> OR --to <upperbound>
    Stop to count the candidates before position <upperbound>. The upper limit
    will not be counted as part of the list, meaning that "-t 10" will count
    all the candidates from position 0 to position 9 (first 10 candidates).
    The value should be positive and greater than the lower bound (if any).    
   
    The <candidates.xml> file must be valid XML (mwttoolkit-candidates.dtd). 
You must chose either the -y option or the -i otpion, both are not allowed at 
the same time. 
"""    
get_freq_function = None
get_ngram_freq_function = None
cache_file = None
freq_name = "?"
ignore_pos = False
web_freq = None
the_corpus_size = -1
temp_candidates = None
sentence_list = {}
lower_limit = -1
upper_limit = -1
candidate_counter = 0
sentence_counter = 0
     
################################################################################
       
def treat_meta( meta ) :
    """
        Adds a `CorpusSize` meta-information to the header and prints the 
        header. The corpus size is important to allow the calculation of 
        statistical Association Measures by the `feat_association.py` script.
        
        @param meta The `Meta` header that is being read from the XML file.        
    """
    global freq_name, the_corpus_size
    meta.add_corpus_size( CorpusSize( name=freq_name, value=the_corpus_size ) )
    print meta.to_xml()
       
################################################################################
       
def treat_candidate( candidate ) :
    """
        For each candidate, searches for the individual word frequencies of the
        base ngram. The corresponding function, for a corpus index or for yahoo,
        will be called, depending on the -i or -y options. The frequencies are
        added as a child element of the word and then the candidate is printed.
        
        @param candidate The `Candidate` that is being read from the XML file.        
    """
    global get_freq_function, freq_name, temp_candidates, sentence_list, \
           upper_limit, lower_limit, candidate_counter
     # Intersection of all sentences that contain the words in the candidate
    if candidate_counter % 100 == 0 :
        verbose( "Processing candidate %(id)d" % { "id":candidate.id_number } )
    if ( upper_limit < 0 or candidate_counter < upper_limit ) and \
        ( lower_limit < 0 or candidate_counter >= lower_limit ) :
        sents_intersect = None
        for word in candidate.base.word_list :
            if word.surface == WILDCARD :
                token = word.lemma
            else :
                token = word.surface
            ( freq_value, sents ) = get_freq_function( token, word.pos )
            word.add_frequency( Frequency( freq_name, freq_value ) )
            # For all word in the candidate, verify in which sentences they
            # occurr and intersect them with sentences in which previous words
            # occurred.
            if sents_intersect is None :
                sents_intersect = sents
            else :
                sents_intersect = sents & sents_intersect
        # put the candidate in a temp file
        temp_candidates[ str( candidate.id_number ) ] = candidate
        # Put the sentence intersection in the sentence list (inverse sent/cand)
        for sent_id in sents_intersect :
            cand_list = sentence_list.get( sent_id, [] )
            cand_list.append( candidate.id_number )
            sentence_list[ sent_id ] = cand_list        
    candidate_counter = candidate_counter + 1

################################################################################
       
def treat_sentence( sentence ) :
    """
        For each sentence in the corpus, 
        
        @param sentence A `Sentence` that is being read from the XML file.    
    """       
    global sentence_list, temp_candidates, ignore_pos, freq_name, \
           sentence_counter
    
    if sentence_counter % 100 == 0 :
        verbose( "Processing sentence %(id)d" % { "id": sentence_counter } )

    for cand_id in sentence_list.get( sentence.s_id, [] ) :
        candidate = temp_candidates[ str( cand_id ) ]
        freq_value = candidate.base.get_freq_value( freq_name )
        freq_value = freq_value + sentence.count( candidate.base, ignore_pos )
        candidate.base.update_freq_value( freq_name, freq_value )
        temp_candidates[ str( cand_id ) ] = candidate
        
    sentence_counter = sentence_counter + 1        
       
################################################################################       
       
def get_freq_index( token, pos ) :
    """
        Gets the frequency (number of occurrences) of a token (word) in the
        index file. Calling this function assumes that you called the script
        with the -i option and with a valid index file.
        
        @param token A string corresponding to the surface form or lemma
        of a word.
        
        @param pos A string corresponding to the Part Of Speech of a word.
    """
    global ignore_pos
    pos_dict = cache_file.get( token.encode( 'utf-8' ) , {} )
    if ignore_pos :
        # Make the union of all POS
        sents_union = set([])
        total_freq = 0
        # pdb.set_trace()
        for pos in pos_dict.keys() :
            if pos != 0 :
                total_freq = total_freq + pos_dict[ pos ]
            else :
                sents_union = sents_union | pos_dict[ pos ]
            
        return (total_freq, sents_union)
    else :
        ( freq_pos, sents ) = pos_dict.get( pos, ( 0, set([]) ) )
        return ( freq_pos, sents )
    
################################################################################

def get_freq_web( token, pos ) : 
    """
        Gets the frequency (number of occurrences) of a token (word) in the
        Web through Yahoo's or Google's index. Calling this function assumes 
        that you called the script with the -y or -w option.
        
        @param token A string corresponding to the surface form or lemma
        of a word.
        
        @param pos A string corresponding to the Part Of Speech of a word. This
        parameter is ignored since Web search engines dos no provide linguistic 
        information.
    """
    # POS is ignored
    global web_freq
    return ( web_freq.search_frequency( token ), set([]) )

################################################################################   

def get_ngram_freq_index() :
    """
    """
    global temp_candidates, freq_name
    
    # Read the corpus sentence by sentence and update the ngram frequencies of the cands
    input_file = open( freq_name + ".xml" )    
    parser = xml.sax.make_parser()
    parser.setContentHandler( CorpusXMLHandler( treat_sentence ) ) 
    parser.parse( input_file )
    input_file.close()   

################################################################################  

def get_ngram_freq_web() :
    """
    """
    global temp_candidates
    
    for cand_id in temp_candidates.keys() :
        candidate = temp_candidates[ cand_id ]       
        candidate_string = ""     
        for word in candidate.base.word_list :
            if word.surface == WILDCARD :
                token = word.lemma
            else :
                token = word.surface
            candidate_string = candidate_string + " " + token        
        (freq_value, sents) = get_freq_function( candidate_string.strip(),None )
        candidate.base.add_frequency( Frequency( freq_name, freq_value ) ) 
        temp_candidates[ cand_id ] = candidate     
    
################################################################################    
    
def print_all_candidates() : 
    """
    """
    global temp_candidates
    
    for cand_id in sorted( map( lambda a:int(a), temp_candidates.keys() ) ) :
        print temp_candidates[ str( cand_id ) ] .to_xml().encode( 'utf-8' )    
    
################################################################################

def open_index( index_filename ) :
    """
        Open the index file (a valid index created by the `index.py` script). 
        The index is not loaded into main memory, only opened as a shelve 
        object (something like a very simple embedded DB). Meta-information 
        about the corpus size and name is retrieved.
        
        @param index_filename The string name of the index file.
    """
    global cache_file, freq_name, the_corpus_size
    try :
        cache_file = shelve.open( index_filename )
        freq_name = cache_file[ INDEX_NAME_KEY ]
        the_corpus_size = cache_file[ CORPUS_SIZE_KEY ]
        # Test if the corpus file still exists
        if not os.path.exists( freq_name + ".xml" ) :
            print >> sys.stderr, "The system wasn't able to locate the " + \
                                 "original corpus file, called " + \
                                 "%(corpus_name)s." % { "corpus_name" : \
                                 freq_name + ".xml" }
            print >> sys.stderr, "Please verify if you moved, renamed or " + \
                                 "deleted the original corpus file used to " + \
                                 "create this index. Remember that an index "+ \
                                 "file is useless if you don't keep the " + \
                                 "original corpus file." 
            sys.exit( 2 )        
    except IOError :        
        print >> sys.stderr, "Error opening the index."
        print >> sys.stderr, "Try again with another index filename."
        sys.exit( 2 )
    except KeyError :        
        print >> sys.stderr, "Error opening the index."
        print >> sys.stderr, "Try again with another index filename."
        sys.exit( 2 )        

################################################################################  

def treat_options( opts, arg, n_arg, usage_string ) :
    """
        Callback function that handles the command line options of this script.
        
        @param opts The options parsed by getopts. Ignored.
        
        @param arg The argument list parsed by getopts.
        
        @param n_arg The number of arguments expected for this script.    
    """
    global cache_file, get_freq_function, get_ngram_freq_function, freq_name, \
           ignore_pos, web_freq, the_corpus_size, lower_limit, upper_limit
    mode = []
    for ( o, a ) in opts:
        if o in ( "-i", "--index" ) : 
            open_index( a )
            get_freq_function = get_freq_index
            get_ngram_freq_function = get_ngram_freq_index
            mode.append( "index" )              
        elif o in ( "-y", "--yahoo" ) :
            web_freq = YahooFreq()          
            freq_name = "yahoo"
            the_corpus_size = web_freq.corpus_size()         
            get_freq_function = get_freq_web
            get_ngram_freq_function = get_ngram_freq_web
            mode.append( "yahoo" )   
        elif o in ( "-w", "--google" ) :
            web_freq = GoogleFreq()          
            freq_name = "google"
            the_corpus_size = web_freq.corpus_size()         
            get_freq_function = get_freq_web
            get_ngram_freq_function = get_ngram_freq_web            
            mode.append( "google" )   
        elif o in ("-g", "--ignore-pos"): 
            ignore_pos = True        
        elif o in ( "-f", "--from" ) :
            try :
                lower_limit = int( a )
            except TypeError :
                print >> sys.stderr, "The argument of -f must be an integer"
                sys.exit( -1 )
        elif o in ( "-t", "--to" ) :
            try :
                upper_limit = int( a )
            except TypeError :
                print >> sys.stderr, "The argument of -t must be an integer"
                sys.exit( -1 )                
        elif o in ("-v", "--verbose") :
            set_verbose( True )
            verbose( "Verbose mode on" )                
    if len(mode) != 1 :
        print >> sys.stderr, "Exactly one option, -y or -i, must be provided"
        usage( usage_string )
        sys.exit( 2 )
                
    treat_options_simplest( opts, arg, n_arg, usage_string )

################################################################################   
# MAIN SCRIPT

longopts = [ "yahoo", "google", "index=", "ignore-pos", "from=", "to=", \
             "--verbose" ]
arg = read_options( "ywi:gf:t:v", longopts, treat_options, 1, usage_string )  

try :    
    # Try to create a temporary file for the cands    
    try :    
        temp_fh = tempfile.NamedTemporaryFile( prefix=TEMP_PREFIX, 
                                               dir=TEMP_FOLDER )
        temp_name = temp_fh.name
        temp_fh.close()
        temp_candidates = shelve.open( temp_name, 'n' )
    except IOError, err :
        print >> sys.stderr, err
        print >> sys.stderr, "Error opening temporary file."
        print >> sys.stderr, "Please verify __common.py configuration"
        sys.exit( 2 )


    print """<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE candidates SYSTEM "dtd/mwttoolkit-candidates.dtd">
<candidates>
""" 
    input_file = open( arg[ 0 ] )        
    parser = xml.sax.make_parser()
    parser.setContentHandler(CandidatesXMLHandler(treat_meta, treat_candidate)) 
    parser.parse( input_file )
    input_file.close()
    verbose( "Getting overall frequencies from the corpus" )
    get_ngram_freq_function() 
    verbose( "Outputting candidates stored in temp file" ) 
    print_all_candidates()  
        
    try :
        temp_candidates.close()
        os.remove( temp_name )
    except IOError, err :
        print >> sys.stderr, err
        print >> sys.stderr, "Error closing temporary file. " + \
              "Please verify __common.py configuration"        
        sys.exit( 2 )
     
    print "</candidates>"        
     
except IOError, err :
    print >> sys.stderr, err
#except Exception, err :
#    print >> sys.stderr, err
#    print >> sys.stderr, "You probably provided an invalid candidates file," + \
#                         " please validate it against the DTD " + \
#                         "(mwttoolkit-candidates.dtd)"
